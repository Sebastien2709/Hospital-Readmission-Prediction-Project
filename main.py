"""
Hospital Readmission Prediction Project
---------------------------------------
A Machine Learning pipeline to predict hospital readmissions within 30 days
for diabetic patients.

Author: [Ton Nom/Pseudo]
Dataset: UCI Diabetes 130-US Hospitals
Model: HistGradientBoostingClassifier (Optimized for tabular data with NaNs)
Performance: ~0.68 AUC-ROC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

from sklearn.model_selection import train_test_split
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import class_weight

# --- CONFIGURATION ---
DATA_PATH = 'data/diabetic_data.csv' # Assure-toi que le fichier est au m√™me endroit
MODEL_SAVE_PATH = 'readmission_model.pkl'
RANDOM_SEED = 42

def load_and_clean_data(filepath):
    """
    Charge les donn√©es et effectue le nettoyage initial critique.
    """
    print("üì• Chargement des donn√©es...")
    df = pd.read_csv(filepath)
    
    # 1. Gestion des valeurs manquantes standardis√©es
    df = df.replace('?', np.nan)
    
    # 2. Suppression des d√©c√®s et hospices
    # On ne peut pas pr√©dire une r√©admission si le patient d√©c√®de ou part en soins palliatifs.
    # IDs: 11, 13, 14, 19, 20, 21
    excl_ids = [11, 13, 14, 19, 20, 21]
    initial_len = len(df)
    df = df[~df['discharge_disposition_id'].isin(excl_ids)]
    print(f"   - Suppression des patients d√©c√©d√©s/hospice: {initial_len - len(df)} lignes retir√©es.")
    
    # 3. Cr√©ation de la cible (Target)
    # 1 = R√©admission < 30 jours, 0 = Sinon (NO ou >30)
    df['target'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)
    
    return df

def feature_engineering(df):
    """
    Transforme les donn√©es brutes en features utilisables par le mod√®le.
    """
    print("‚öôÔ∏è Feature Engineering en cours...")
    
    # 1. Mapping de l'√¢ge (Intervalles -> Num√©rique)
    age_map = {
        '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35, 
        '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75, 
        '[80-90)': 85, '[90-100)': 95
    }
    df['age_num'] = df['age'].map(age_map).fillna(70)
    
    # 2. Gestion des variables Cat√©gorielles
    # HistGradientBoosting g√®re bien les cat√©gories encod√©es en entiers (0, 1, 2...)
    # On utilise LabelEncoder pour tout transformer en chiffres.
    
    cat_cols = [
        'race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 
        'medical_specialty', 'diag_1', 'diag_2', 'diag_3', 
        'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed'
    ]
    
    # Ajout des m√©dicaments √† la liste cat√©gorielle
    meds = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 
            'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 
            'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 
            'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 
            'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone']
    cat_cols.extend(meds)
    
    # On garde une trace des encodeurs si on veut inverser plus tard (optionnel)
    encoders = {} 
    
    # Identification des colonnes √† "Faible Cardinalit√©" pour le support natif HGB
    # HGB (sklearn) accepte max 255 cat√©gories. Au-dessus, on traite comme du num√©rique ordinal.
    low_card_cols_indices = []
    
    # S√©lection des colonnes finales
    num_cols = ['time_in_hospital', 'num_lab_procedures', 'num_procedures', 
                'num_medications', 'number_outpatient', 'number_emergency', 
                'number_inpatient', 'number_diagnoses', 'age_num']
    
    final_cols = num_cols + cat_cols
    df_model = df[final_cols].copy()
    
    # Encodage
    for i, col in enumerate(final_cols):
        if col in cat_cols:
            df_model[col] = df_model[col].astype(str)
            le = LabelEncoder()
            df_model[col] = le.fit_transform(df_model[col])
            
            # Si < 255 valeurs uniques, on dit au mod√®le "C'est une cat√©gorie !"
            if df_model[col].nunique() <= 255:
                low_card_cols_indices.append(i)
                
    return df_model, df['target'], low_card_cols_indices

def train_evaluate_model(X, y, cat_indices):
    """
    Entra√Æne le mod√®le HistGradientBoosting et √©value les performances.
    """
    print("üöÄ Entra√Ænement du mod√®le (HistGradientBoosting)...")
    
    # Split Train/Test (Stratifi√© pour garder la proportion de r√©admissions)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED
    )
    
    # Calcul des poids pour g√©rer le d√©s√©quilibre (moins de classe 1 que de 0)
    sample_weights = class_weight.compute_sample_weight(
        class_weight='balanced', y=y_train
    )
    
    # Mod√®le : HistGradientBoosting (Rapide et g√®re les NaNs et Cat√©gories)
    model = HistGradientBoostingClassifier(
        learning_rate=0.05,
        max_iter=500,
        max_depth=12,
        l2_regularization=1.0,
        categorical_features=cat_indices, # Support natif
        early_stopping=True,
        random_state=RANDOM_SEED
    )
    
    model.fit(X_train, y_train, sample_weight=sample_weights)
    
    # Pr√©dictions
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    y_pred_class = (y_pred_proba > 0.5).astype(int)
    
    # --- M√©triques ---
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"\nüéØ R√âSULTAT FINAL - AUC Score: {auc:.4f}")
    print("\nRapport de Classification :")
    print(classification_report(y_test, y_pred_class))
    
    return model, X_test, y_test, y_pred_proba

def plot_results(model, X_test, y_test, y_pred_proba):
    """
    G√©n√®re les graphiques pour l'analyse.
    """
    # 1. Courbe ROC
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'Model AUC = {roc_auc_score(y_test, y_pred_proba):.2f}', color='teal', linewidth=2)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('Taux de Faux Positifs')
    plt.ylabel('Taux de Vrais Positifs')
    plt.title('Courbe ROC - Pr√©diction de R√©admission')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.savefig('roc_curve_final.png')
    print("üìä Graphique ROC sauvegard√© sous 'roc_curve_final.png'")
    
    # 2. Matrice de Confusion
    cm = confusion_matrix(y_test, (y_pred_proba > 0.5).astype(int))
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Pr√©dit')
    plt.ylabel('R√©el')
    plt.title('Matrice de Confusion')
    plt.savefig('confusion_matrix_final.png')

def main():
    # 1. Load
    df = load_and_clean_data(DATA_PATH)
    
    # 2. Prepare
    X, y, cat_indices = feature_engineering(df)
    
    # 3. Train
    model, X_test, y_test, y_probs = train_evaluate_model(X, y, cat_indices)
    
    # 4. Visualize
    plot_results(model, X_test, y_test, y_probs)
    
    # 5. Save
    joblib.dump(model, MODEL_SAVE_PATH)
    print(f"üíæ Mod√®le sauvegard√© sous '{MODEL_SAVE_PATH}'")

if __name__ == "__main__":
    main()